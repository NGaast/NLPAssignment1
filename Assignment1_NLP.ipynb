{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gaastra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_df = pd.read_csv(\"Data/AAPL.csv\")\n",
    "news_df = pd.read_csv(\"Data/us_equities_news_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in news_df 'content' column: 516\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Steps\n",
    "\n",
    "# Apple dataframe\n",
    "# Add price_higher column based on:\n",
    "# Close > Open = 1\n",
    "# Close <= Open = 0\n",
    "apple_df['price_higher'] = apple_df.apply(lambda row: 1 if row['Close'] > row['Open'] else 0, axis=1)\n",
    "\n",
    "# News dataframe\n",
    "# Find and delete duplicate content\n",
    "print(f\"Duplicate rows in news_df 'content' column: {news_df['content'].duplicated().sum()}\")\n",
    "news_df.drop_duplicates('content', inplace=True)\n",
    "\n",
    "# Filter news content based on Apple stock\n",
    "news_df = news_df[news_df['ticker'] == 'AAPL']\n",
    "\n",
    "# Join price_higher column with news dataframe based on date\n",
    "news_df = news_df.merge(apple_df[['Date', 'price_higher']], left_on='release_date', right_on='Date')\n",
    "news_df = news_df.drop(columns=['Date'])\n",
    "\n",
    "# Convert release_date to datetime type\n",
    "news_df['release_date'] = pd.to_datetime(news_df['release_date'])\n",
    "\n",
    "#\n",
    "news_df['content'] = news_df['content'].apply(lambda x: '. '.join(x.split(' \\n')))\n",
    "news_df['content'] = news_df['content'].apply(lambda x: '. '.join(list(filter(None, [i.strip(' ') for i in x.split('\\r\\n')]))))\n",
    "news_df['words_amount'] = news_df.apply(lambda row: len(row['content'].split(\" \")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of Apple articles is: 17624\n",
      "The average amount of words per article is: 723\n",
      "The amount unique words for all articles are: 137077\n",
      "9529 8095\n"
     ]
    }
   ],
   "source": [
    "print(\"The amount of Apple articles is: \" + str(len(news_df['article_id'].unique())))\n",
    "print(\"The average amount of words per article is: \" + str(round(news_df['words_amount'].mean())))\n",
    "print(\"The amount unique words for all articles are: \" + str(len(set(' '.join(news_df['content']).split(' ')))))\n",
    "\n",
    "\n",
    "# TEST: Distribution of price_higher\n",
    "days_higher = news_df[news_df['price_higher'] == 1]['price_higher'].count()\n",
    "days_lower = news_df[news_df['price_higher'] == 0]['price_higher'].count()\n",
    "\n",
    "print(days_higher, days_lower)\n",
    "#Lexical richness bekijk https://pypi.org/project/lexicalrichness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and aggregate content by date\n",
    "news_df.groupby('release_date').agg({'content': '.'.join})\n",
    "news_df['content'] = news_df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        jpmorgan lifts apple aapl target ahead tomorro...\n",
       "1        kim investing com faang stocks predictably str...\n",
       "2        chuck mikolajczak new york reuters u stocks su...\n",
       "3        two best performing tech stocks set report res...\n",
       "4        yasin ebrahim kim apple readies earnings inves...\n",
       "                               ...                        \n",
       "17619    stock market difficult one traders investors a...\n",
       "17620    tsx index leading canadian stocks outperformed...\n",
       "17621    europe flares summer heat continues summer hea...\n",
       "17622    last quarter apple aapl reported best quarter ...\n",
       "17623    may look like spider web mishmash trendlines m...\n",
       "Name: content, Length: 17624, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "test_news_df = news_df.copy()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    test = [token for token in tokens if (token != '' and token not in english_stopwords) and token.isalpha()]\n",
    "    return test\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "words_to_be_removed = ['apple', 'aapl']\n",
    "\n",
    "# Tokenize content string\n",
    "test_news_df['content'] = test_news_df['content'].apply(tokenize)\n",
    "# Remove stopwords, empty tokens and punctuation/numbers\n",
    "test_news_df['content'] = test_news_df['content'].apply(clean_tokens)\n",
    "# Join tokens into string\n",
    "test_news_df['content'] = test_news_df['content'].apply(join_tokens)\n",
    "test_news_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [i.strip(' ') for i in news_df['content'][3].split('\\r\\n')]\n",
    "test = list(filter(None, test))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
